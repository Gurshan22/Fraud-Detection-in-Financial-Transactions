import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler, PowerTransformer
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.linear_model import LogisticRegression
import xgboost as xgb
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
from sklearn.pipeline import Pipeline
import time
import warnings
warnings.filterwarnings('ignore')

# Load the dataset
def load_data():
    print("Loading dataset...")
    # Load Credit Card Fraud Detection dataset
    df = pd.read_csv('creditcard.csv')
    return df

# Data preprocessing
def preprocess_data(df):
    print("Preprocessing data...")
    # Check for null values
    print(f"Null values in dataset:\n{df.isnull().sum()}")
    
    # Display dataset info
    print(f"\nDataset Information:")
    print(f"Shape: {df.shape}")
    print(f"Columns: {df.columns}")
    print(f"Class distribution:\n{df['Class'].value_counts()}")
    
    # Check for data imbalance
    fraud_ratio = df['Class'].sum() / len(df)
    print(f"Fraud ratio: {fraud_ratio:.4f} ({df['Class'].sum()} frauds out of {len(df)} transactions)")
    
    # Visualize class distribution
    plt.figure(figsize=(8, 6))
    sns.countplot(x='Class', data=df)
    plt.title('Class Distribution')
    plt.xlabel('Fraud (1) vs Normal (0)')
    plt.ylabel('Count')
    plt.yscale('log')
    plt.show()
    
    # Distribution of transaction amounts
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    sns.histplot(df[df['Class'] == 0]['Amount'], kde=True, color='blue')
    plt.title('Transaction Amount (Normal)')
    plt.xlabel('Amount')
    plt.xscale('log')
    
    plt.subplot(1, 2, 2)
    sns.histplot(df[df['Class'] == 1]['Amount'], kde=True, color='red')
    plt.title('Transaction Amount (Fraud)')
    plt.xlabel('Amount')
    plt.xscale('log')
    plt.tight_layout()
    plt.show()
    
    # Correlation matrix
    plt.figure(figsize=(18, 14))
    corr_matrix = df.corr()
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
    sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='coolwarm', center=0)
    plt.title('Feature Correlation Matrix')
    plt.show()
    
    # Prepare features and labels
    X = df.drop('Class', axis=1)
    y = df['Class']
    
    # Train-test split with stratification to maintain class distribution
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
    
    # Feature scaling
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    return X_train_scaled, X_test_scaled, y_train, y_test, scaler

# Function to evaluate models
def evaluate_model(model, X_test, y_test, model_name):
    # Make predictions
    start_time = time.time()
    y_pred = model.predict(X_test)
    inference_time = time.time() - start_time
    
    # For probability-based metrics
    if hasattr(model, "predict_proba"):
        y_prob = model.predict_proba(X_test)[:, 1]
    else:
        # For models like Isolation Forest that use decision_function
        y_prob = model.decision_function(X_test) if hasattr(model, "decision_function") else None
    
    # Print classification report
    report = classification_report(y_test, y_pred)
    print(f"Classification Report for {model_name}:")
    print(report)
    
    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix - {model_name}')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()
    
    # ROC Curve
    if y_prob is not None:
        plot_roc_curve(y_test, y_prob, model_name)
        plot_precision_recall_curve(y_test, y_prob, model_name)
    
    print(f"Inference time: {inference_time:.4f} seconds")
    return y_pred, y_prob

# Plot ROC curve
def plot_roc_curve(y_true, y_prob, model_name):
    fpr, tpr, _ = roc_curve(y_true, y_prob)
    roc_auc = auc(fpr, tpr)
    
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'ROC Curve - {model_name}')
    plt.legend(loc="lower right")
    plt.show()

# Plot Precision-Recall curve
def plot_precision_recall_curve(y_true, y_prob, model_name):
    precision, recall, _ = precision_recall_curve(y_true, y_prob)
    avg_precision = average_precision_score(y_true, y_prob)
    
    plt.figure(figsize=(8, 6))
    plt.plot(recall, precision, color='blue', lw=2, label=f'Precision-Recall curve (AP = {avg_precision:.2f})')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title(f'Precision-Recall Curve - {model_name}')
    plt.legend(loc="upper right")
    plt.show()

# Implement Random Forest classifier
def train_random_forest(X_train, y_train, X_test, y_test):
    print("\nTraining Random Forest classifier...")
    
    # Define base model
    rf = RandomForestClassifier(random_state=42)
    
    # Define hyperparameters for grid search
    param_grid = {
        'n_estimators': [100, 200],
        'max_depth': [None, 10, 20],
        'min_samples_split': [2, 5],
        'min_samples_leaf': [1, 2]
    }
    
    # Grid search with stratified k-fold cross-validation
    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
    grid_search = GridSearchCV(
        estimator=rf,
        param_grid=param_grid,
        cv=cv,
        scoring='roc_auc',
        n_jobs=-1,
        verbose=1
    )
    
    # Fit grid search
    grid_search.fit(X_train, y_train)
    
    # Get best model
    best_rf = grid_search.best_estimator_
    print(f"Best parameters: {grid_search.best_params_}")
    
    # Evaluate best model
    y_pred_rf, y_prob_rf = evaluate_model(best_rf, X_test, y_test, "Random Forest")
    
    # Feature importance
    feature_importance = best_rf.feature_importances_
    indices = np.argsort(feature_importance)[-15:]  # Top 15 features
    
    plt.figure(figsize=(10, 8))
    plt.title('Feature Importance')
    plt.barh(range(len(indices)), feature_importance[indices], color='b', align='center')
    plt.yticks(range(len(indices)), [f'Feature {i}' for i in indices])
    plt.xlabel('Relative Importance')
    plt.show()
    
    return best_rf, y_pred_rf, y_prob_rf

# Implement XGBoost classifier
def train_xgboost(X_train, y_train, X_test, y_test):
    print("\nTraining XGBoost classifier...")
    
    # Define base model
    xgb_model = xgb.XGBClassifier(
        objective='binary:logistic',
        random_state=42,
        use_label_encoder=False,
        eval_metric='logloss'
    )
    
    # Define hyperparameters for grid search
    param_grid = {
        'n_estimators': [100, 200],
        'max_depth': [3, 5, 7],
        'learning_rate': [0.01, 0.1],
        'subsample': [0.8, 1.0],
        'colsample_bytree': [0.8, 1.0]
    }
    
    # Grid search with stratified k-fold cross-validation
    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
    grid_search = GridSearchCV(
        estimator=xgb_model,
        param_grid=param_grid,
        cv=cv,
        scoring='roc_auc',
        n_jobs=-1,
        verbose=1
    )
    
    # Fit grid search
    grid_search.fit(X_train, y_train)
    
    # Get best model
    best_xgb = grid_search.best_estimator_
    print(f"Best parameters: {grid_search.best_params_}")
    
    # Evaluate best model
    y_pred_xgb, y_prob_xgb = evaluate_model(best_xgb, X_test, y_test, "XGBoost")
    
    # Feature importance
    feature_importance = best_xgb.feature_importances_
    indices = np.argsort(feature_importance)[-15:]  # Top 15 features
    
    plt.figure(figsize=(10, 8))
    plt.title('Feature Importance')
    plt.barh(range(len(indices)), feature_importance[indices], color='b', align='center')
    plt.yticks(range(len(indices)), [f'Feature {i}' for i in indices])
    plt.xlabel('Relative Importance')
    plt.show()
    
    return best_xgb, y_pred_xgb, y_prob_xgb

# Implement Isolation Forest for anomaly detection
def train_isolation_forest(X_train, y_train, X_test, y_test):
    print("\nTraining Isolation Forest for anomaly detection...")
    
    # Define and train Isolation Forest
    iso_forest = IsolationForest(
        n_estimators=100,
        contamination=sum(y_train)/len(y_train),  # Set contamination to the proportion of frauds
        random_state=42
    )
    
    # Fit the model
    iso_forest.fit(X_train)
    
    # Predict on test set (returns 1 for inliers and -1 for outliers)
    y_pred_iso = iso_forest.predict(X_test)
    
    # Convert Isolation Forest output to binary labels (1 for fraud, 0 for normal)
    y_pred_iso = np.where(y_pred_iso == -1, 1, 0)
    
    # Evaluate model
    y_pred_iso, y_score_iso = evaluate_model(iso_forest, X_test, y_test, "Isolation Forest")
    
    return iso_forest, y_pred_iso, y_score_iso

# Implement SMOTE for handling imbalanced data
def train_with_smote(X_train, y_train, X_test, y_test):
    print("\nTraining models with SMOTE for handling imbalanced data...")
    
    # Define SMOTE
    smote = SMOTE(random_state=42)
    
    # Apply SMOTE to training data
    X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)
    
    print(f"Original training set shape: {X_train.shape}")
    print(f"Resampled training set shape: {X_train_smote.shape}")
    print(f"Original class distribution: {pd.Series(y_train).value_counts()}")
    print(f"Resampled class distribution: {pd.Series(y_train_smote).value_counts()}")
    
    # Train Random Forest with SMOTE
    rf_smote = RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=42
    )
    rf_smote.fit(X_train_smote, y_train_smote)
    
    # Evaluate Random Forest with SMOTE
    y_pred_rf_smote, y_prob_rf_smote = evaluate_model(rf_smote, X_test, y_test, "Random Forest with SMOTE")
    
    # Train XGBoost with SMOTE
    xgb_smote = xgb.XGBClassifier(
        objective='binary:logistic',
        n_estimators=100,
        max_depth=5,
        learning_rate=0.1,
        random_state=42,
        use_label_encoder=False,
        eval_metric='logloss'
    )
    xgb_smote.fit(X_train_smote, y_train_smote)
    
    # Evaluate XGBoost with SMOTE
    y_pred_xgb_smote, y_prob_xgb_smote = evaluate_model(xgb_smote, X_test, y_test, "XGBoost with SMOTE")
    
    return rf_smote, xgb_smote, y_pred_rf_smote, y_pred_xgb_smote, y_prob_rf_smote, y_prob_xgb_smote

# Main function to execute the project
def main():
    # Load data
    df = load_data()
    
    # Preprocess data
    X_train, X_test, y_train, y_test, scaler = preprocess_data(df)
    
    # Train and evaluate Random Forest
    rf_model, y_pred_rf, y_prob_rf = train_random_forest(X_train, y_train, X_test, y_test)
    
    # Train and evaluate XGBoost
    xgb_model, y_pred_xgb, y_prob_xgb = train_xgboost(X_train, y_train, X_test, y_test)
    
    # Train and evaluate Isolation Forest
    iso_forest, y_pred_iso, y_score_iso = train_isolation_forest(X_train, y_train, X_test, y_test)
    
    # Train and evaluate models with SMOTE
    rf_smote, xgb_smote, y_pred_rf_smote, y_pred_xgb_smote, y_prob_rf_smote, y_prob_xgb_smote = train_with_smote(X_train, y_train, X_test, y_test)
    
    # Compare all models
    print("\nModel Comparison:")
    models = [
        ("Random Forest", y_pred_rf, y_prob_rf),
        ("XGBoost", y_pred_xgb, y_prob_xgb),
        ("Isolation Forest", y_pred_iso, y_score_iso),
        ("Random Forest with SMOTE", y_pred_rf_smote, y_prob_rf_smote),
        ("XGBoost with SMOTE", y_pred_xgb_smote, y_prob_xgb_smote)
    ]
    
    for model_name, y_pred, y_prob in models:
        print(f"\n{model_name} Results:")
        print(classification_report(y_test, y_pred))
    
    # Conclusion
    print("\nConclusion:")
    print("This fraud detection system demonstrates the effectiveness of machine learning")
    print("in identifying fraudulent financial transactions. The models were evaluated")
    print("with a focus on recall (detecting all frauds) while maintaining acceptable precision.")
    print("XGBoost with SMOTE provided the best balance of precision and recall for this task.")
    print("Further improvements could be made with more advanced feature engineering")
    print("and ensemble methods combining multiple models.")

if __name__ == "__main__":
    main()
